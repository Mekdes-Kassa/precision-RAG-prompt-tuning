{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv()\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    openai_api_key=openai_key, \n",
    "    model='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand string theory.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"String theory is a theoretical framework in physics that aims to describe the fundamental nature of particles and forces in the universe. It suggests that at the most fundamental level, everything in the universe is made up of tiny, vibrating strings of energy.\\n\\nIn traditional particle physics, elementary particles like electrons and quarks are considered point-like particles with no internal structure. However, in string theory, these particles are replaced by tiny, one-dimensional strings. The vibrations of these strings give rise to different particles and their properties, such as mass and charge.\\n\\nOne of the key ideas in string theory is that it requires more than the usual four dimensions (three spatial dimensions and one time dimension) of spacetime. In fact, string theory predicts the existence of extra dimensions, which are compactified or curled up in such a way that they are not directly observable at our macroscopic scale.\\n\\nString theory also attempts to unify all the fundamental forces of nature, including gravity, into a single consistent framework. It suggests that these forces arise from different vibrational modes of the strings. This unification could potentially resolve some of the long-standing problems in physics, such as the incompatibility between general relativity (which describes gravity) and quantum mechanics (which describes the other fundamental forces).\\n\\nIt's worth noting that string theory is a highly complex and mathematical theory, and it is still an active area of research with many open questions. Nonetheless, it has provided valuable insights and connections between different branches of physics.\\n\\nIf you're interested in learning more, there are many books and online resources available that delve deeper into the intricacies of string theory.\")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physicists believe that string theory has the potential to provide a unified theory for several reasons:\n",
      "\n",
      "1. Consistency with quantum mechanics: String theory is inherently a quantum theory, meaning it describes particles and forces in a way consistent with the principles of quantum mechanics. This is important because quantum mechanics successfully describes the behavior of particles at the microscopic level. By incorporating quantum mechanics, string theory offers a framework that can potentially unify all fundamental forces, including gravity.\n",
      "\n",
      "2. Gravity and the other forces: One of the main challenges in physics is reconciling gravity, described by Einstein's theory of general relativity, with the other three fundamental forces (electromagnetism, strong nuclear force, and weak nuclear force), which are described by quantum field theory. String theory provides a way to describe gravity in a quantum framework, potentially allowing for the unification of gravity with the other forces.\n",
      "\n",
      "3. Extra dimensions: String theory predicts the existence of extra dimensions beyond our familiar three spatial dimensions and one time dimension. These extra dimensions could provide a geometric framework for unifying the forces. By compactifying these dimensions in a specific way, different vibrational modes of the strings could correspond to different particles and forces.\n",
      "\n",
      "4. Dualities and symmetries: String theory exhibits various dualities and symmetries, such as T-duality and supersymmetry. These mathematical symmetries suggest that seemingly different string theories might be different descriptions of the same underlying physical reality. These dualities and symmetries provide valuable insights into the connections between different branches of physics and hint at a deeper underlying unity.\n",
      "\n",
      "It's important to note that while string theory shows promise in unifying fundamental forces, it is still a work in progress. Many aspects of the theory are still being explored, and there are ongoing debates and challenges. However, physicists continue to study string theory in the hopes of finding a unified description of the universe.\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Why do physicists believe it can produce a 'unified theory'?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the challenge document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import os \n",
    "\n",
    "# Open the PDF file in binary read mode\n",
    "with open(\"/home/hp/Documents/week 5/precision-RAG-prompt-tuning/prompt_generation/challenge.pdf\",'rb') as file:\n",
    "    # Create a PDF reader object\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "\n",
    "    for page in range(num_pages):\n",
    "        page_obj = pdf_reader.pages[page]\n",
    "        extracted_text = page_obj.extract_text()\n",
    "        \n",
    "len(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: References\n",
      "●\n",
      "Meistrari\n",
      "didn’t\n",
      "see\n",
      "a\n",
      "good\n",
      "solution\n",
      "\n",
      "Chunk 2: see\n",
      "a\n",
      "good\n",
      "solution\n",
      "for\n",
      "prompt\n",
      "engineering,\n",
      "so\n",
      "it’\n",
      "Chunk 3: \n",
      "engineering,\n",
      "so\n",
      "it’s\n",
      "building\n",
      "one\n",
      "●\n",
      "AutoPrompt:\n",
      "E\n",
      "Chunk 4: \n",
      "one\n",
      "●\n",
      "AutoPrompt:\n",
      "Eliciting\n",
      "Knowledge\n",
      "from\n",
      "Langua\n",
      "Chunk 5: nowledge\n",
      "from\n",
      "Language\n",
      "Models\n",
      "with\n",
      "Automatically\n",
      "G\n",
      "Chunk 6: with\n",
      "Automatically\n",
      "Generated\n",
      "Prompts\n",
      "●\n",
      "Large\n",
      "Langu\n",
      "Chunk 7: rompts\n",
      "●\n",
      "Large\n",
      "Language\n",
      "Models\n",
      "Are\n",
      "Human-Level\n",
      "Pro\n",
      "Chunk 8: \n",
      "Are\n",
      "Human-Level\n",
      "Prompt\n",
      "Engineers\n",
      "●\n",
      "Prompt\n",
      "Enginee\n",
      "Chunk 9: ers\n",
      "●\n",
      "Prompt\n",
      "Engineering\n",
      "●\n",
      "How\n",
      "to\n",
      "Create\n",
      "a\n",
      "Monte\n",
      "C\n",
      "Chunk 10: \n",
      "to\n",
      "Create\n",
      "a\n",
      "Monte\n",
      "Carlo\n",
      "Simulation\n",
      "using\n",
      "Python\n",
      "●\n",
      "Chunk 11: ation\n",
      "using\n",
      "Python\n",
      "●\n",
      "Monte\n",
      "Carlo\n",
      "Method\n",
      "Explained\n",
      "\n",
      "Chunk 12: lo\n",
      "Method\n",
      "Explained\n",
      "●\n",
      "What\n",
      "is\n",
      "Monte\n",
      "Carlo\n",
      "Simulati\n",
      "Chunk 13: Monte\n",
      "Carlo\n",
      "Simulation?\n",
      "How\n",
      "does\n",
      "it\n",
      "work?\n",
      "●\n",
      "Elo\n",
      "Ra\n",
      "Chunk 14: es\n",
      "it\n",
      "work?\n",
      "●\n",
      "Elo\n",
      "Rating\n",
      "Algorithm\n",
      "●\n",
      "Elo\n",
      "algorithm\n",
      "Chunk 15: ithm\n",
      "●\n",
      "Elo\n",
      "algorithm\n",
      "implementation\n",
      "in\n",
      "Python\n",
      "●\n",
      "Tr\n",
      "Chunk 16: ation\n",
      "in\n",
      "Python\n",
      "●\n",
      "TrueSkillTM:\n",
      "A\n",
      "Bayesian\n",
      "skill\n",
      "ra\n",
      "Chunk 17: \n",
      "A\n",
      "Bayesian\n",
      "skill\n",
      "rating\n",
      "system\n",
      "Chunk 18: m\n"
     ]
    }
   ],
   "source": [
    "chunk_size=50\n",
    "overlap=20\n",
    "def chunk_text(extracted_text, chunk_size, overlap):\n",
    "    chunks = []\n",
    "    text_length = len(extracted_text)\n",
    "\n",
    "    for start in range(0, text_length, chunk_size - overlap):\n",
    "        end = min(start + chunk_size, text_length)\n",
    "        chunk = extracted_text[start:end]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(extracted_text, chunk_size, overlap)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i + 1}:\", chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "References\n",
      "●\n",
      "Meistrari\n",
      "didn’t\n",
      "see\n",
      "a\n",
      "good\n",
      "solution\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = chunk_text(extracted_text, chunk_size, overlap)\n",
    "\n",
    "#for i, chunk in enumerate(chunks):\n",
    "    #print(f\"Chunk {i + 1}:\", chunk)\n",
    "print(chunks[0])\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize the client and connect to the targetted index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=\"cc6b9914-0016-4fa4-ab44-f82fe08434b9\")\n",
    "index = pc.Index(\"mekdes-index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embed and add chunks to pinecone index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 1536)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = embed_model.embed_documents(chunks)\n",
    "len(res), len(res[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import hashlib \n",
    "\n",
    "def generate_unique_id_for_chunk(chunk):\n",
    "    # Generate a unique ID using a combination of timestamp and hash of the chunk content\n",
    "    timestamp = str(time.time())  # Current timestamp\n",
    "    chunk_hash = hashlib.sha256(chunk.encode()).hexdigest()  # Hash of the chunk content\n",
    "    unique_id = f\"{timestamp}_{chunk_hash}\"\n",
    "    \n",
    "    return unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over chunks and corresponding embeddings to upsert into Pinecone\n",
    "for chunk, embedding in zip(chunks, res):\n",
    "    document_id = generate_unique_id_for_chunk(chunk) \n",
    "    index.upsert(vectors=[(document_id, embedding)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the nearest neighbors of a vector\n",
    "index_name = \"mekdes-index\"\n",
    "query_vector = [0.1, 0.2, 0.3]  # Replace with your desired vector\n",
    "top_k = 5  # Number of nearest neighbors to retrieve\n",
    "import pinecone \n",
    "index = pinecone.Index(index_name, host='https://mekdes-index-nn3xpxm.svc.gcp-starter.pinecone.io')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"  # the metadata field that contains our text\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_from_vector(chunks, user_question):\n",
    "    prompt = chunks + \" \\n based on the above data, give an answer to \\\n",
    "            the following question. restrict yourself to the above data only. \\\n",
    "            if you can't get an answer based on the data, you can feel free to \\\n",
    "                say i don't know. here is the question. \\n\" + user_question\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    return np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U33'), dtype('<U23')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is so special about Llama 2?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[83], line 4\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(embedding1, embedding2)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcosine_similarity\u001b[39m(embedding1, embedding2):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding2\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(embedding1) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(embedding2))\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U33'), dtype('<U23')) -> None"
     ]
    }
   ],
   "source": [
    "query = \"What is so special about Llama 2?\"\n",
    "\n",
    "cosine_similarity(query, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(request):\n",
    "    if request.method == 'POST':\n",
    "        input_text = request.POST.get('input_text')\n",
    "        # Embed the user's input\n",
    "        embeded_question = embed_text([input_text])[0]\n",
    "        highest_similarity = -1\n",
    "        best_text_chunk = None\n",
    "        # Compare with embeddings in TextChunk objects\n",
    "        for text_chunk in chunks.objects.all():\n",
    "            similarity = cosine_similarity(embeded_question, text_chunk.embed)\n",
    "            if similarity > highest_similarity:\n",
    "                highest_similarity = similarity\n",
    "                best_text_chunk = text_chunk.chunk\n",
    "        if best_text_chunk is not None:\n",
    "            generated_prompt = generate_prompt_from_vector(best_text_chunk, input_text)\n",
    "            # return HttpResponse(generated_prompt)\n",
    "            return render(request, 'prompt_app/prompt_result.html', {'generated_prompt': generated_prompt})\n",
    "        else:\n",
    "            return HttpResponse(\"No similar documents found.\")\n",
    "    else:\n",
    "        return render(request, 'prompt_app/generate_prompt.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
